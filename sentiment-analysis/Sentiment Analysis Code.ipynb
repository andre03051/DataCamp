{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from langdetect import detect_langs\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "movies = pd.read_csv('data\\IMDB_sample.csv')\n",
    "\n",
    "# Find the number of positive and negative reviews\n",
    "print('Number of positive and negative reviews: \\n', movies.label.value_counts())\n",
    "\n",
    "# Find the proportion of positive and negative reviews\n",
    "print('Proportion of positive and negative reviews: \\n', movies.label.value_counts() / len(movies))\n",
    "\n",
    "length_reviews = movies.review.str.len()\n",
    "\n",
    "# How long is the longest review\n",
    "print(max(length_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "two_cities = 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair, we had everything before us, we had nothing before us, we were all going direct to Heaven, we were all going direct the other way – in short, the period was so far like the present period, that some of its noisiest authorities insisted on its being received, for good or for evil, in the superlative degree of comparison only.'\n",
    "annak = 'Happy families are all alike; every unhappy family is unhappy in its own way'\n",
    "catcher = \"If you really want to hear about it,the first thing you'll probably want to know is where I was born, and what my lousy childhood was like, and how my parents were occupied and all before they had me, and all that David Copperfield kind of crap, but I don't feel like going into it, if you want to know the truth.\"\n",
    "with open('data/titanic.txt', errors='ignore') as f:\n",
    "    titanic = f.read()\n",
    "\n",
    "# Create a textblob object \n",
    "blob_two_cities = TextBlob(two_cities)\n",
    "blob_annak = TextBlob(annak)\n",
    "blob_catcher = TextBlob(catcher)\n",
    "blob_titanic = TextBlob(titanic)\n",
    "\n",
    "\n",
    "# Print out the sentiment   \n",
    "print('Sentiment of annak: \\n', blob_annak.sentiment)\n",
    "print('Sentiment of catcher: \\n', blob_catcher.sentiment)\n",
    "print('Sentiment of titanic: \\n', blob_titanic.sentiment)\n",
    "print('Sentiment of titanic: \\n', blob_two_cities.sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Build the vectorizer and fit it\n",
    "anna_vect = CountVectorizer()\n",
    "anna_vect.fit(annak)\n",
    "\n",
    "# Create the bow representation\n",
    "anna_bow = anna_vect.transform(annak)\n",
    "\n",
    "# Print the bag-of-words result \n",
    "print(anna_bow.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "reviews = pd.read_csv('data/amazon_reviews_sample.csv')\n",
    "\n",
    "# Build the vectorizer, specify max features \n",
    "vect = CountVectorizer(max_features=100)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review column\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_df=pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   able to  about how  about it  about the  about this  after reading  \\\n",
      "0        0          0         0          0           0              0   \n",
      "1        0          0         0          0           0              0   \n",
      "2        0          0         0          0           0              0   \n",
      "3        0          0         0          0           0              0   \n",
      "4        0          0         0          0           0              0   \n",
      "\n",
      "   after the  again and  ago and  agree with  ...  you think  you to  you ve  \\\n",
      "0          0          0        0           0  ...          0       0       0   \n",
      "1          0          0        0           0  ...          0       0       0   \n",
      "2          0          0        0           0  ...          0       0       2   \n",
      "3          0          0        0           0  ...          0       0       0   \n",
      "4          0          0        0           0  ...          0       0       1   \n",
      "\n",
      "   you want  you will  you won  you would  your money  your own  your time  \n",
      "0         0         0        0          0           0         0          0  \n",
      "1         0         0        0          0           0         0          0  \n",
      "2         0         0        0          0           0         0          0  \n",
      "3         0         0        0          0           0         0          0  \n",
      "4         0         0        0          0           0         0          0  \n",
      "\n",
      "[5 rows x 1000 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "reviews = pd.read_csv('data/amazon_reviews_sample.csv')\n",
    "\n",
    "# Build the vectorizer, specify max features and fit\n",
    "vect = CountVectorizer(max_features=1000, ngram_range=(2, 2), max_df=500)\n",
    "vect.fit(reviews.review)\n",
    "\n",
    "# Transform the review\n",
    "X_review = vect.transform(reviews.review)\n",
    "\n",
    "# Create a DataFrame from the bow representation\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GoT = 'Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'\n",
    "\n",
    "print(word_tokenize(GoT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avengers = [\"Cause if we can't protect the Earth, you can be d*** sure we'll avenge it\",\n",
    " 'There was an idea to bring together a group of remarkable people, to see if we could become something more',\n",
    " \"These guys come from legend, Captain. They're basically Gods.\"]\n",
    "\n",
    " # Tokenize each item in the avengers \n",
    "tokens_avengers = [word_tokenize(item) for item in avengers]\n",
    "\n",
    "print(tokens_avengers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "reviews = pd.read_csv('data/amazon_reviews_sample.csv')\n",
    "\n",
    "# Tokenize each item in the review column \n",
    "word_tokens = [word_tokenize(review) for review in reviews.review]\n",
    "\n",
    "# Create an empty list to store the length of reviews\n",
    "len_tokens = []\n",
    "\n",
    "# Iterate over the word_tokens list and determine the length of each item\n",
    "for i in range(len(word_tokens)):\n",
    "     len_tokens.append(len(word_tokens[i]))\n",
    "\n",
    "# Create a new feature for the lengh of each review\n",
    "reviews['n_words'] = len_tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foreign = \"L'histoire rendu était fidèle, excellent, et grande.\"\n",
    "# Detect the language of the foreign string\n",
    "print(detect_langs(foreign))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"L'histoire rendu était fidèle, excellent, et grande.\",\n",
    " 'Excelente muy recomendable.',\n",
    " 'It had a leak from day one but the return and exchange process was very quick.']\n",
    "\n",
    "languages = []\n",
    "\n",
    "# Loop over the sentences in the list and detect their language\n",
    "for sentence in sentences:\n",
    "    languages.append(detect_langs(sentence))\n",
    "\n",
    "print('The detected languages are: ', languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('data/amazon_reviews_sample.csv')\n",
    "\n",
    "languages = [] \n",
    "\n",
    "# Loop over the rows of the dataset and append  \n",
    "for row in range(len(reviews)):\n",
    "    languages.append(detect_langs(reviews.iloc[row, 2]))\n",
    "\n",
    "# Clean the list by splitting     \n",
    "languages = [str(lang).split(':')[0][1:] for lang in languages]\n",
    "\n",
    "# Assign the list to a new feature \n",
    "reviews['language'] = languages\n",
    "\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "tweets = pd.read_csv('data/Tweets.csv')\n",
    "\n",
    "# Define the stop words\n",
    "my_stop_words = ENGLISH_STOP_WORDS.union(['airline', 'airlines', '@'])\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(tweets.text)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(tweets.text)\n",
    "\n",
    "# Create the data frame\n",
    "X_df = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "print(X_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment of GoT: \n",
      " Sentiment(polarity=0.5, subjectivity=0.8888888888888888)\n",
      "Time taken for stemming in seconds:  0.003000974655151367\n",
      "Stemmed tokens:  ['never', 'forget', 'what', 'you', 'are', ',', 'for', 'sure', 'the', 'world', 'will', 'not', '.', 'make', 'it', 'your', 'strength', '.', 'then', 'it', 'can', 'never', 'be', 'your', 'weak', '.', 'armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'use', 'to', 'hurt', 'you', '.']\n",
      "Time taken for lemmatizing in seconds:  0.005000591278076172\n",
      "Lemmatized tokens:  ['Never', 'forget', 'what', 'you', 'are', ',', 'for', 'surely', 'the', 'world', 'will', 'not', '.', 'Make', 'it', 'your', 'strength', '.', 'Then', 'it', 'can', 'never', 'be', 'your', 'weakness', '.', 'Armour', 'yourself', 'in', 'it', ',', 'and', 'it', 'will', 'never', 'be', 'used', 'to', 'hurt', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "# TextBlob’s output for a polarity task is a float within the range [-1.0, 1.0] where -1.0 is a negative polarity and 1.0 is positive. This score can also be equal to 0, which stands for a neutral evaluation of a statement as it doesn’t contain any words from the training set.\n",
    "# Whereas, a subjectivity/objectivity identification task reports a float within the range [0.0, 1.0] where 0.0 is a very objective sentence and 1.0 is very subjective\n",
    "\n",
    "GoT = 'Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armour yourself in it, and it will never be used to hurt you.'\n",
    "\n",
    "blob_GoT = TextBlob(GoT)\n",
    "print('Sentiment of GoT: \\n', blob_GoT.sentiment)\n",
    "\n",
    "porter = PorterStemmer()\n",
    "WNlemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Tokenize the GoT string\n",
    "tokens = word_tokenize(GoT) \n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a stemmed list\n",
    "stemmed_tokens = [porter.stem(token) for token in tokens] \n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for stemming in seconds: ', end_time - start_time)\n",
    "print('Stemmed tokens: ', stemmed_tokens) \n",
    "\n",
    "# Log the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Build a lemmatized list\n",
    "lem_tokens = [WNlemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Log the end time\n",
    "end_time = time.time()\n",
    "\n",
    "print('Time taken for lemmatizing in seconds: ', end_time - start_time)\n",
    "print('Lemmatized tokens: ', lem_tokens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tweet_id']\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "tweets = pd.read_csv('data/Tweets.csv')\n",
    "\n",
    "# Call the stemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "# Transform the array of tweets to tokens\n",
    "tokens = [word_tokenize(tweet) for tweet in tweets]\n",
    "\n",
    "# Stem the list of tokens\n",
    "stemmed_tokens = [[porter.stem(word) for word in tweet] for tweet in tokens] \n",
    "\n",
    "# Print the first element of the list\n",
    "print(stemmed_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.4472136  0.4472136  0.         0.4472136  0.\n",
      "  0.4472136  0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.30151134 0.         0.30151134\n",
      "  0.         0.30151134 0.30151134 0.30151134 0.30151134 0.60302269\n",
      "  0.30151134]]\n"
     ]
    }
   ],
   "source": [
    "annak = ['Happy families are all alike;', 'every unhappy family is unhappy in its own way']\n",
    "\n",
    "# Call the vectorizer and fit it\n",
    "anna_vect = TfidfVectorizer().fit(annak)\n",
    "\n",
    "# Create the tfidf representation\n",
    "anna_tfidf = anna_vect.transform(annak)\n",
    "\n",
    "# Print the result \n",
    "print(anna_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "tweets = pd.read_csv('data/Tweets.csv')\n",
    "\n",
    "# Define the vectorizer and specify the arguments\n",
    "my_pattern = r'\\b[^\\d\\W][^\\d\\W]+\\b'\n",
    "vect = TfidfVectorizer(\n",
    "    ngram_range=(1, 2), \n",
    "    max_features=100, \n",
    "    token_pattern=my_pattern, \n",
    "    stop_words=ENGLISH_STOP_WORDS\n",
    ").fit(tweets.text)\n",
    "\n",
    "# Transform the vectorizer\n",
    "X_txt = vect.transform(tweets.text)\n",
    "\n",
    "# Transform to a data frame and specify the column names\n",
    "X=pd.DataFrame(X_txt.toarray(), columns=vect.get_feature_names())\n",
    "print('Top 5 rows of the DataFrame: \\n', X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "movies = pd.read_csv('data\\IMDB_sample.csv')\n",
    "\n",
    "# Build and fit the vectorizer\n",
    "vect = CountVectorizer(stop_words=my_stop_words)\n",
    "vect.fit(movies.review)\n",
    "\n",
    "# Create the bow representation\n",
    "X_review = vect.transform(movies.review)\n",
    "\n",
    "# Create the data frame\n",
    "movies = pd.DataFrame(X_review.toarray(), columns=vect.get_feature_names())\n",
    "\n",
    "# Define the vector of labels and matrix of features\n",
    "y = movies.label\n",
    "X = movies.drop('label', axis=1)\n",
    "\n",
    "# Perform the train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a logistic regression model and print out the accuracy\n",
    "log_reg = LogisticRegression().fit(X_train, y_train)\n",
    "print('Accuracy on train set: ', log_reg.score(X_train, y_train))\n",
    "print('Accuracy on test set: ', log_reg.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "468f25ad0239460415b7e6b7483d5c8f7213894121f6fb96c4cb6ef93fffe534"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
