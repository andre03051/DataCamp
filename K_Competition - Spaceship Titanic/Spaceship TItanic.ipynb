{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_heat_scatter(clf, param_grid, n_iter, X_train, y_train, X_test):\n",
    "    # Create a random search object\n",
    "    rand_search = RandomizedSearchCV(\n",
    "        estimator = clf,\n",
    "        param_distributions = param_grid,\n",
    "        n_iter = n_iter,\n",
    "        scoring='roc_auc', \n",
    "        n_jobs=4, \n",
    "        cv = 5, \n",
    "        refit=True, \n",
    "        return_train_score = True\n",
    ")\n",
    "\n",
    "    # Fit to the training data\n",
    "    rand_search.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "    # Create dataframe of random grid search results\n",
    "    params = list(param_grid.keys())\n",
    "    heatdf = pd.DataFrame()\n",
    "    for p in params:\n",
    "        heatdf[p] = rand_search.cv_results_['param_' + p].tolist()\n",
    "\n",
    "    heatdf['score'] = rand_search.cv_results_['mean_test_score'].tolist()\n",
    "    heatdf.dropna(inplace=True)\n",
    "\n",
    "    variables = list(range(0, len(params)))\n",
    "\n",
    "    if variables > 2:\n",
    "        dependent = []\n",
    "        independent = []\n",
    "\n",
    "        for v in variables:\n",
    "            idx = 0\n",
    "            while idx <= max(variables):\n",
    "                if v < idx:\n",
    "                    dependent.append(params[v])\n",
    "                    independent.append(params[idx])\n",
    "                    idx += 1\n",
    "                else:\n",
    "                    idx += 1\n",
    "\n",
    "        plot_inputs = pd.DataFrame({\"input\":dependent,\"output\":independent})\n",
    "\n",
    "            # Subplots are organized in a Rows x Cols Grid\n",
    "        Tot = plot_inputs.shape[0]\n",
    "        Cols = np.ceil(Tot/4).astype(int)\n",
    "\n",
    "        # Compute Rows required\n",
    "        Rows = Tot // Cols \n",
    "        Rows += Tot % Cols\n",
    "\n",
    "        # Create a Position index\n",
    "        Position = range(1,Tot + 1)\n",
    "\n",
    "        cmap = plt.get_cmap(\"Spectral\")\n",
    "        norm = plt.Normalize(heatdf['score'].min(), heatdf['score'].max())\n",
    "\n",
    "        plt.rc('font', size=12)\n",
    "        fig, axs = plt.subplots(Rows, Cols, figsize=(14, 10))\n",
    "\n",
    "        for i, r in plot_inputs.iterrows():\n",
    "            # add every single subplot to the figure with a for loop\n",
    "            # ax = fig.add_subplot(Rows,Cols,Position[i])\n",
    "            axs.ravel()[i].set(xlabel=r['input'], ylabel=r['output'])\n",
    "            axs.ravel()[i].scatter(\n",
    "                x = heatdf[r['input']], \n",
    "                y = heatdf[r['output']],\n",
    "                c = heatdf['score'],\n",
    "                cmap = 'gist_rainbow'\n",
    "            )      \n",
    "            axs.ravel()[i].set(xlabel=r['input'], ylabel=r['output'])\n",
    "\n",
    "        sm =  ScalarMappable(norm=norm, cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        cbar = fig.colorbar(sm, ax=axs)\n",
    "        cbar.ax.set_title(\"scale\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.rc('font', size=16)\n",
    "        plt.figure(figsize = (12,6))\n",
    "        plt.scatter(\n",
    "            x = heatdf[r['input']], \n",
    "            y = heatdf[r['output']],\n",
    "            c = heatdf['score'],\n",
    "            cmap = 'gist_rainbow'\n",
    "        )\n",
    "        plt.xlabel(r['input'])\n",
    "        plt.ylabel(r['output'])\n",
    "        plt.colorbar()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>Group</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>TotalSpent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0001</td>\n",
       "      <td>01</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0002</td>\n",
       "      <td>01</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0003</td>\n",
       "      <td>01</td>\n",
       "      <td>10383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0003</td>\n",
       "      <td>02</td>\n",
       "      <td>5176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0004</td>\n",
       "      <td>01</td>\n",
       "      <td>1091.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CryoSleep   Age  VIP  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  \\\n",
       "0          0  39.0    0          0.0        0.0           0.0     0.0     0.0   \n",
       "1          0  24.0    0        109.0        9.0          25.0   549.0    44.0   \n",
       "2          0  58.0    1         43.0     3576.0           0.0  6715.0    49.0   \n",
       "3          0  33.0    0          0.0     1283.0         371.0  3329.0   193.0   \n",
       "4          0  16.0    0        303.0       70.0         151.0   565.0     2.0   \n",
       "\n",
       "   Transported Group GroupNumber  TotalSpent  \n",
       "0            0  0001          01         0.0  \n",
       "1            1  0002          01       736.0  \n",
       "2            0  0003          01     10383.0  \n",
       "3            0  0003          02      5176.0  \n",
       "4            1  0004          01      1091.0  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Break up stacked features\n",
    "train[['Group','GroupNumber']]=train.PassengerId.str.split('_',expand=True)\n",
    "train.drop('PassengerId', axis=1, inplace=True)\n",
    "\n",
    "# train[['Deck','CabinNumber', 'Side']]=train.Cabin.str.split('/',expand=True)\n",
    "train.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "# # Convert boolean to numbers\n",
    "train.CryoSleep = train.CryoSleep*1\n",
    "train.VIP = train.VIP*1\n",
    "train.Transported = train.Transported*1\n",
    "\n",
    "# Drop useless features\n",
    "train.drop('Name', axis=1, inplace=True)\n",
    "train.drop('Destination', axis = 1, inplace = True)\n",
    "train.drop('HomePlanet', axis = 1, inplace = True)\n",
    "# train.drop('CryoSleep', axis = 1, inplace = True)\n",
    "# train.drop('VIP', axis = 1, inplace = True)\n",
    "\n",
    "# Fill NA money columns with 0\n",
    "train[['RoomService','FoodCourt','ShoppingMall','Spa', 'VRDeck', 'CryoSleep', 'VIP']] = \\\n",
    "    train[['RoomService','FoodCourt','ShoppingMall','Spa', 'VRDeck', 'CryoSleep', 'VIP']].fillna(value=0)\n",
    "\n",
    "# Makeup some new features\n",
    "train['TotalSpent'] = train['RoomService'] + train['FoodCourt'] + \\\n",
    "    train['ShoppingMall'] + train['Spa'] + train['VRDeck']\n",
    "\n",
    "# Sneak Peak!\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CryoSleep         0\n",
       "Age             179\n",
       "VIP               0\n",
       "RoomService       0\n",
       "FoodCourt         0\n",
       "ShoppingMall      0\n",
       "Spa               0\n",
       "VRDeck            0\n",
       "Transported       0\n",
       "Group             0\n",
       "GroupNumber       0\n",
       "TotalSpent        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What data is missing?\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump rows of missing data interpolation or filling doesn't make sense\n",
    "train.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Transported</th>\n",
       "      <th>Group</th>\n",
       "      <th>GroupNumber</th>\n",
       "      <th>TotalSpent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>736.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10383.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1091.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CryoSleep   Age  VIP  RoomService  FoodCourt  ShoppingMall     Spa  VRDeck  \\\n",
       "0          0  39.0    0          0.0        0.0           0.0     0.0     0.0   \n",
       "1          0  24.0    0        109.0        9.0          25.0   549.0    44.0   \n",
       "2          0  58.0    1         43.0     3576.0           0.0  6715.0    49.0   \n",
       "3          0  33.0    0          0.0     1283.0         371.0  3329.0   193.0   \n",
       "4          0  16.0    0        303.0       70.0         151.0   565.0     2.0   \n",
       "\n",
       "   Transported  Group  GroupNumber  TotalSpent  \n",
       "0            0      1            1         0.0  \n",
       "1            1      2            1       736.0  \n",
       "2            0      3            1     10383.0  \n",
       "3            0      3            2      5176.0  \n",
       "4            1      4            1      1091.0  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct data types\n",
    "train = train.astype({\n",
    "    \"Group\": int, \n",
    "    \"GroupNumber\": int,\n",
    "    \"CryoSleep\":int,\n",
    "    \"VIP\":int,\n",
    "    \"Transported\":int\n",
    "})\n",
    "\n",
    "# Create some dummies\n",
    "train = pd.get_dummies(train, drop_first = True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic</td>\n",
       "      <td>0.812683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GradientBoost</td>\n",
       "      <td>0.804463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.801527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.792132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.773341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model     Score\n",
       "0       Logistic  0.812683\n",
       "2  GradientBoost  0.804463\n",
       "3       AdaBoost  0.801527\n",
       "5        XGBoost  0.792132\n",
       "4   RandomForest  0.773341"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create features and predictor\n",
    "X = train.drop('Transported', axis=1)\n",
    "y = train['Transported']\n",
    "\n",
    "# Traing, test, and split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=123)\n",
    "\n",
    "# Create classifiers\n",
    "lr = LogisticRegression(max_iter = 500)\n",
    "gnb = GaussianNB()\n",
    "ada = AdaBoostClassifier()\n",
    "grb = GradientBoostingClassifier()\n",
    "rfc = RandomForestClassifier()\n",
    "xgbc = xgb.XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric = 'logloss')\n",
    "\n",
    "clf_list = [\n",
    "    (lr, \"Logistic\"),\n",
    "    (gnb, \"NaiveBayes\"),\n",
    "    (grb, 'GradientBoost'),\n",
    "    (ada, 'AdaBoost'),\n",
    "    (rfc, \"RandomForest\"),\n",
    "    (xgbc, 'XGBoost')\n",
    "]\n",
    "\n",
    "s = []\n",
    "m = []\n",
    "for i, (clf, name) in enumerate(clf_list):\n",
    "    p = clf.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "    s.append(accuracy_score(y_test, p.round()))\n",
    "    m.append(name)\n",
    "\n",
    "modelSelection = pd.DataFrame(m, columns=['Model'])\n",
    "modelSelection['Score'] = s\n",
    "modelSelection.sort_values(\"Score\", inplace=True, ascending=False)\n",
    "\n",
    "modelSelection.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "190 fits failed out of a total of 5000.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "190 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 681, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1589, in fit\n",
      "    fold_coefs_ = Parallel(\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 211, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 864, in _logistic_regression_path\n",
      "    w0, n_iter_i, warm_start_sag = sag_solver(\n",
      "  File \"c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py\", line 327, in sag_solver\n",
      "    num_seen, n_iter_ = sag(\n",
      "  File \"sklearn\\linear_model\\_sag_fast.pyx\", line 620, in sklearn.linear_model._sag_fast.sag64\n",
      "ValueError: Floating-point under-/overflow occurred at epoch #1. Scaling input data with StandardScaler or MinMaxScaler might help.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.8071912  0.80554546 0.80624267 0.80584773 0.80671609 0.80686569\n",
      " 0.8069394  0.80692906 0.8067911  0.80699028 0.80586152 0.80542647\n",
      " 0.80644362 0.80580676 0.80606331 0.8067467  0.80508242 0.8065833\n",
      " 0.80660959 0.80547086 0.8068644  0.80553296 0.80582315 0.80685232\n",
      " 0.80689716 0.80676997 0.80574855 0.80705324 0.80677083 0.80650528\n",
      " 0.80660528 0.80626726 0.80706488 0.80677256 0.80559073 0.80604304\n",
      " 0.80568645 0.80660916 0.80531997 0.80676782 0.80543035 0.80694544\n",
      " 0.80628364 0.8055446  0.80593826 0.80561272 0.80585721 0.80602363\n",
      " 0.80603053 0.80607452 0.80700408 0.80601759 0.80551096 0.8056386\n",
      " 0.80634488 0.80556659 0.80548552 0.80508113 0.80660485 0.80654623\n",
      " 0.80587488 0.80721707 0.80687647 0.80595163 0.80683939 0.80522383\n",
      " 0.80547906 0.80562221        nan 0.80669324 0.80625949 0.80603183\n",
      " 0.80651217 0.8069338  0.80555754 0.80620559 0.80505397 0.80582616\n",
      " 0.80511044 0.80643413 0.80678248 0.80511043 0.80570629 0.80708989\n",
      " 0.80701357 0.80544802 0.80631469 0.80548337 0.80617455 0.80522771\n",
      "        nan 0.80720414 0.80652899 0.80708817 0.80616593 0.80626812\n",
      " 0.80589342 0.80589342 0.80574984 0.8068644  0.80559073 0.805157\n",
      " 0.80547388 0.80689457 0.8061642  0.8053937  0.80638885 0.80568257\n",
      " 0.80613316 0.80669496 0.80580805 0.80589428 0.80561013 0.80576882\n",
      " 0.80647854 0.80567869 0.80631211 0.80693941 0.80660528 0.80629485\n",
      " 0.8060784  0.80526738 0.80567093 0.80688768 0.80622371 0.80671393\n",
      " 0.80532342 0.80539111 0.8065458  0.80532859 0.80525918 0.80709162\n",
      " 0.80694113 0.80570413 0.806297          nan 0.80650311 0.80602148\n",
      " 0.80606374 0.8059788  0.80669065 0.80651777 0.80567179 0.80599948\n",
      " 0.80608961 0.80639145 0.80701917 0.80550751        nan        nan\n",
      " 0.80683249 0.80553986 0.80543725 0.80677213 0.80518848 0.80667729\n",
      " 0.80657727 0.80599431 0.80683896 0.80601414 0.80694199 0.80658503\n",
      " 0.80560237 0.80565326 0.8064531         nan 0.80715629 0.8061642\n",
      " 0.80581798 0.80709205 0.80601113 0.80720543 0.80676567 0.80716318\n",
      " 0.80597232 0.80579641 0.80667987 0.8058973  0.80574855 0.80560841\n",
      " 0.8050975  0.80691182 0.80551657 0.80616981 0.8065902  0.80587704\n",
      " 0.80654321 0.80651433 0.80714335 0.80715327 0.80676954 0.80601846\n",
      " 0.80536093 0.80647638 0.80667858 0.80677299 0.80717568 0.80629571\n",
      " 0.80711749 0.80687    0.80520357 0.80686827 0.80650786 0.80689285\n",
      " 0.80537257 0.80703384 0.80714292 0.80697045 0.80505095 0.80565929\n",
      " 0.80630003 0.80626596 0.8064656  0.80685017 0.80709765 0.80641602\n",
      " 0.80694329 0.80596672 0.80683939 0.80512294 0.80571146 0.80662598\n",
      " 0.80713344 0.80512552        nan 0.80631383 0.80660184 0.80635609\n",
      " 0.80676696 0.80713387        nan 0.80690837 0.80553166 0.80536179\n",
      " 0.80676739 0.80705195 0.80514148 0.80658589 0.80629442 0.80539499\n",
      " 0.80546741 0.80620904 0.80626898 0.80661304 0.80505095 0.80657727\n",
      " 0.80588954 0.80683724 0.80545578 0.8062375  0.80616808 0.80664064\n",
      " 0.80511431 0.80589557        nan 0.80517425 0.80602924 0.80531221\n",
      " 0.80513889 0.80687991 0.80674023        nan 0.80714637 0.80678938\n",
      " 0.80541138 0.80711447 0.80572785 0.80570629 0.80676954 0.80626855\n",
      " 0.80681998        nan 0.80650657 0.8064587  0.80700365 0.80640525\n",
      " 0.80589428 0.80693984 0.8052234  0.80658287 0.80509104 0.80563041\n",
      " 0.80530531 0.80649622 0.8061849  0.80614221 0.80607021 0.80650786\n",
      " 0.80625259 0.80593223 0.80611074 0.80610945 0.80613833 0.80606676\n",
      " 0.80690837        nan        nan 0.80657123 0.80572656 0.80563817\n",
      " 0.8070942  0.80581064 0.80692691 0.80709033 0.80548768        nan\n",
      " 0.8051113  0.80600595 0.80636644 0.80540017 0.80715543 0.80620516\n",
      " 0.80563645 0.80643801 0.80695104 0.80632073 0.80587833 0.80631555\n",
      " 0.80706272 0.80665444 0.80650484 0.80616463 0.80574768 0.80576321\n",
      " 0.80613876 0.80572742 0.80715758 0.80546138 0.80515959 0.80618705\n",
      " 0.80623405 0.80523202 0.80676782 0.80582875 0.80715586 0.80572871\n",
      " 0.80527427 0.8058986  0.80631512 0.80638541        nan 0.80554805\n",
      " 0.80635436 0.80667556 0.80543768 0.80641645 0.80575976 0.80512423\n",
      " 0.80682732 0.80605857 0.80611979 0.8063259  0.80602579 0.80708774\n",
      " 0.80572957 0.80555452 0.8063535  0.80709205 0.80624656 0.80678464\n",
      " 0.80701271 0.80581711 0.80689587 0.8060176  0.80616291 0.80530833\n",
      "        nan 0.806507   0.80551225 0.80619783 0.80616463 0.80542819\n",
      " 0.80624871 0.80606978 0.80683766 0.80650743 0.80721707 0.80635091\n",
      " 0.80511259 0.80692734 0.80712482 0.80633323 0.80643241 0.80665573\n",
      " 0.80606633 0.80686741 0.80629657 0.80662598 0.80715413 0.80583263\n",
      " 0.80527988 0.80594171 0.80545708 0.80607495 0.80533463 0.80676222\n",
      " 0.80504836 0.80650527 0.80608702 0.80655873 0.8060577  0.80549673\n",
      " 0.80543509 0.80667729 0.80553382 0.80716836 0.80705798 0.80570973\n",
      " 0.80640136 0.80629787 0.80693337 0.80687431 0.80545622 0.80693035\n",
      "        nan 0.80698338 0.80596715 0.80609005 0.80560668 0.806319\n",
      " 0.80701831 0.80593912 0.80551743 0.80686526 0.80668591 0.80676782\n",
      " 0.80621293 0.80627846 0.80721449 0.80528031 0.80691268 0.80662641\n",
      " 0.80626984 0.80554374 0.8065846  0.80622112 0.8059775  0.80694889\n",
      " 0.80704462 0.80587143 0.8067342  0.80715586 0.80580029 0.80582444\n",
      " 0.80646258 0.80650743 0.8061711  0.80627286 0.80677989        nan\n",
      " 0.80595249 0.80707437 0.805301   0.80652899 0.8068963  0.80686569\n",
      " 0.80510009 0.80709291 0.80667039 0.80674497 0.8060163  0.8065126\n",
      " 0.80718042 0.80629269 0.80684801 0.80650528 0.80511259 0.80696355\n",
      " 0.80505138 0.8057382  0.80590592 0.80620559 0.80667211 0.80672213\n",
      " 0.80717224 0.80677773 0.8060452  0.80505181 0.80517942 0.80538508\n",
      " 0.80623707 0.8067273  0.80531566 0.80507207 0.80663546 0.8059042\n",
      "        nan 0.80663503        nan 0.80651217 0.80691139 0.80629658\n",
      " 0.80521047 0.80604994 0.80686397 0.80549846 0.80639188 0.80694027\n",
      " 0.80660658 0.80668591 0.80589903 0.80694544        nan 0.80676567\n",
      " 0.80671221 0.80594559 0.80600725 0.80718775 0.80601889 0.80689543\n",
      " 0.80707954 0.80659235 0.80687819 0.80601846 0.80660529 0.80528074\n",
      " 0.80648544 0.80572225 0.80561186 0.80719594 0.80567782        nan\n",
      " 0.80635307 0.80625302 0.80703858 0.80523331 0.80531436 0.8050932\n",
      " 0.80508328 0.80721104 0.80630434 0.80548638 0.80662425 0.80614221\n",
      " 0.80616852 0.80567869 0.80513587 0.80602061 0.80694242 0.80689328\n",
      " 0.80663288 0.80527858 0.80718991 0.80585807 0.80619654 0.80694242\n",
      " 0.80718861 0.80609047 0.80639102 0.80532514 0.80562869 0.80715586\n",
      " 0.80509147 0.80605598 0.80568947 0.80582401 0.80526565 0.80510225\n",
      " 0.80631426 0.80568171 0.80642594 0.80660485 0.80664236 0.80682128\n",
      " 0.80720543 0.80608228 0.80662339 0.80699718 0.80715672 0.80667298\n",
      " 0.80650441 0.80693897 0.80602277 0.80524107 0.80717956        nan\n",
      " 0.80613704 0.80719077 0.80650398 0.80571189 0.80685664 0.80588523\n",
      " 0.80602277 0.80629312 0.80647121 0.80690578 0.80704419 0.80639231\n",
      " 0.80629571 0.80717353 0.80704548 0.80630218 0.80656994 0.80684499\n",
      " 0.80715543 0.80631512 0.80602191 0.80579468 0.80532643 0.80577787\n",
      " 0.80645353 0.80613876 0.80718473 0.80660658 0.80624354 0.80675834\n",
      " 0.80581193 0.80718516 0.80674972 0.8068575  0.80687517 0.8068838\n",
      " 0.80677299 0.80569335 0.8063259  0.80720155 0.80691312 0.80590032\n",
      " 0.80720241 0.80714594 0.80683594 0.8069019  0.80563041 0.80603485\n",
      " 0.80680921 0.80559288        nan 0.80713128 0.80585549 0.80619481\n",
      " 0.80539111 0.80678248 0.80672643 0.80589299 0.80658589 0.8064143\n",
      "        nan 0.8057977  0.80616851 0.80590032 0.8055308  0.80688681\n",
      " 0.80571664 0.80571146 0.80682042 0.80546095        nan 0.80655571\n",
      " 0.80554288 0.80645396 0.80579641 0.80709506 0.8065251  0.80572699\n",
      " 0.80553727 0.80523288 0.80531781 0.8055088         nan 0.80584299\n",
      " 0.80611074 0.8056166  0.80591455 0.80615817 0.80652553 0.80548293\n",
      " 0.80687646 0.8057244         nan 0.80518848 0.80667599 0.8071843\n",
      " 0.80575243 0.80631383 0.80531005 0.80692044 0.80587014 0.80520012\n",
      " 0.80539499 0.80671221 0.80684715        nan 0.80553166 0.80676567\n",
      " 0.80626725 0.80553943 0.80543552 0.80613273 0.80534152 0.80719335\n",
      " 0.80593654 0.80616895 0.80687991 0.8060715  0.80691613 0.80561747\n",
      " 0.80651303 0.80613618 0.80677773 0.80709033 0.80546827 0.80520357\n",
      " 0.80641602 0.80557737 0.80638239 0.8055377  0.80719551 0.80704246\n",
      " 0.8069157  0.80681266 0.80523029 0.80660529 0.80589989 0.80619955\n",
      " 0.80551786 0.80510914 0.80693983 0.80598612 0.80684198 0.80613445\n",
      " 0.80545449        nan 0.80540534 0.80564378 0.80532557 0.80518632\n",
      " 0.80584385 0.80666565 0.80616506 0.80668763 0.80629658 0.80611893\n",
      " 0.80692648 0.80719422 0.80541138 0.80681826 0.80538206 0.80693897\n",
      " 0.80513458 0.80682861 0.80605684 0.80562782 0.80589816 0.80563084\n",
      " 0.80694156 0.80621034 0.80722052 0.80651303 0.80691355 0.80626337\n",
      " 0.80602277 0.80626855 0.80681093 0.80705152 0.80649751 0.80672859\n",
      "        nan 0.806845   0.80704375 0.8072106  0.80702047 0.80542129\n",
      " 0.80668893 0.8062996  0.80666694 0.80514881 0.80631426 0.8062914\n",
      " 0.80703858 0.80701616 0.80705281 0.8070472  0.80579813 0.80662857\n",
      " 0.80570887 0.80594559 0.80539111 0.80543423 0.80619956 0.80553296\n",
      " 0.80596672 0.80675101 0.80720414 0.80666263 0.80586238 0.80660485\n",
      "        nan 0.80688768 0.80617283 0.80694156 0.80572742 0.80640568\n",
      " 0.8064281  0.80523116 0.80573087 0.80516045 0.80660528 0.80601242\n",
      " 0.80571146 0.80709162 0.80545449 0.80720586 0.80669496 0.80629916\n",
      " 0.80638842 0.80546225 0.80669539 0.80701702 0.80720586 0.80631426\n",
      " 0.80518115 0.80686397 0.80656649 0.80659494 0.80577658 0.8066553\n",
      " 0.80616636 0.80667255 0.8071537  0.80698726 0.80703944 0.80520616\n",
      "        nan 0.80582616 0.80611979 0.8055765  0.80639101 0.80622284\n",
      " 0.80668893 0.80664237 0.8061448  0.8071537  0.80528375 0.80662555\n",
      " 0.80718473 0.80683853 0.80677989 0.80678119 0.80510828 0.80602191\n",
      " 0.8063203  0.80691958 0.80694975 0.8066747  0.80717999 0.80674497\n",
      " 0.80599388 0.80673937 0.80650829 0.80707523 0.80510656        nan\n",
      " 0.80611764        nan 0.80544888 0.80660571 0.80639834 0.80660701\n",
      " 0.80639921 0.80622155 0.80522857 0.80657037 0.80530876 0.8070416\n",
      " 0.80684586 0.8063134  0.80598095 0.80631124 0.80706315 0.80614178\n",
      " 0.80662598 0.80704591 0.80571966 0.80668807 0.80627674 0.8053937\n",
      " 0.80615041 0.80639058 0.80532342 0.80704936 0.80719336 0.80694975\n",
      " 0.80697691 0.80703384 0.80649751        nan 0.80667556 0.80604865\n",
      " 0.80550751 0.80708903 0.80670099 0.80556831 0.80548552 0.80689759\n",
      " 0.80712784 0.80686311 0.80706962 0.80673894 0.80512423 0.80694587\n",
      " 0.80718861 0.80625906 0.80635825 0.806845   0.8067467  0.8063134\n",
      " 0.80621595 0.80572785 0.80614739 0.80692906 0.80616679 0.80693725\n",
      "        nan 0.80708083 0.80544285 0.80563774 0.80537128 0.80568688\n",
      " 0.80696096 0.805301   0.80532427 0.80651088 0.80649449 0.8055377\n",
      " 0.80680748 0.80562998 0.80632073 0.80661046 0.80663115 0.80506604\n",
      " 0.80641128 0.80686569 0.80718172 0.8053204  0.80596931 0.80617282\n",
      " 0.80653028 0.80662986 0.80682775 0.80719077 0.80559246 0.80616722\n",
      " 0.80709593 0.80704419 0.80545578 0.80564205 0.80540836 0.80509363\n",
      " 0.80635221 0.80721621 0.80607538 0.80554288 0.80666694 0.80671221\n",
      " 0.80617929 0.80542776 0.80635868 0.80554417 0.80689457 0.8071537\n",
      "        nan 0.80650484 0.80604951 0.8070209  0.8052359  0.80709119\n",
      " 0.80653157 0.80663115 0.8061642  0.80604865 0.80614351 0.80722484\n",
      " 0.80573604 0.80587143 0.80643456 0.80546483 0.80631297 0.80517468\n",
      " 0.80507164 0.80701486 0.80538507 0.8056705  0.80650959 0.80687388\n",
      " 0.80633755 0.80528117 0.80710067 0.80609263 0.80646733 0.80676696\n",
      " 0.80512595 0.80649536 0.80667341 0.80647984 0.80513846 0.80541483\n",
      " 0.80638972 0.80543897 0.80542474 0.80686397]\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the train scores are non-finite: [0.80833122 0.80679386 0.80748079 0.80710266 0.8079079  0.80805409\n",
      " 0.8081495  0.80812175 0.80797274 0.80818687 0.8071168  0.80667236\n",
      " 0.80766704 0.80707234 0.80730265 0.80792537 0.80633853 0.80778654\n",
      " 0.80781704 0.80672585 0.80804692 0.80677917 0.80708557 0.80803654\n",
      " 0.80808685 0.80795528 0.80702484 0.80822921 0.80795517 0.80772397\n",
      " 0.80780295 0.80750073 0.80823516 0.80795573 0.80685082 0.80726999\n",
      " 0.80696224 0.80781615 0.80655814 0.80795034 0.80667468 0.80815489\n",
      " 0.80752274 0.8067944  0.8071795  0.80687089 0.80710977 0.80725943\n",
      " 0.8072656  0.80732013 0.80819105 0.80723499 0.80676284 0.8069236\n",
      " 0.80759436 0.80682563 0.80673504 0.80633963 0.80779842 0.80775677\n",
      " 0.80712594 0.80835089 0.80807443 0.80719077 0.80802086 0.80647177\n",
      " 0.80673274 0.80689323        nan 0.80788877 0.80749569 0.8072664\n",
      " 0.80771874 0.80812528 0.8068173  0.80745179 0.80630994 0.80709026\n",
      " 0.8063839  0.80766369 0.80796764 0.80638743 0.80697278 0.80825782\n",
      " 0.80819841 0.80668559 0.80757316 0.80673312 0.80742541 0.80648271\n",
      "        nan 0.80834039 0.80774628 0.80824656 0.80740774 0.80750717\n",
      " 0.80714353 0.8071376  0.80702788 0.80805263 0.80685034 0.80642562\n",
      " 0.80672701 0.80809645 0.80739513 0.80664304 0.80762018 0.80695742\n",
      " 0.80736759 0.80789096 0.80707336 0.80713644 0.80687216 0.80704755\n",
      " 0.80768816 0.80694828 0.80756647 0.80813422 0.80779958 0.80753826\n",
      " 0.80731035 0.8065099  0.80694626 0.80808184 0.80746745 0.80790394\n",
      " 0.80656927 0.80663512 0.80775744 0.80657301 0.80650435 0.80825966\n",
      " 0.80815066 0.80697205 0.80753314        nan 0.80770678 0.80725943\n",
      " 0.80730243 0.80721141 0.80788823 0.80773618 0.80694456 0.80722114\n",
      " 0.80733315 0.80762974 0.80819922 0.80675802        nan        nan\n",
      " 0.80799982 0.80679857 0.80667912 0.80795525 0.80644566 0.80787433\n",
      " 0.80777945 0.80722035 0.80801989 0.80723046 0.80813619 0.80778452\n",
      " 0.80686545 0.80693449 0.80767183        nan 0.80829773 0.80740569\n",
      " 0.80708285 0.80825586 0.80723094 0.80833966 0.80794102 0.80830748\n",
      " 0.80720214 0.80706092 0.80787789 0.80714458 0.80702346 0.80687178\n",
      " 0.806361   0.80810318 0.80677055 0.80741636 0.80779188 0.80712885\n",
      " 0.80775488 0.80771645 0.80828412 0.80829118 0.80795169 0.80725503\n",
      " 0.80660093 0.807687   0.80787813 0.80795781 0.80831341 0.80753258\n",
      " 0.80826998 0.80805821 0.80645854 0.80805767 0.80773214 0.80808416\n",
      " 0.80661585 0.80820989 0.80828307 0.8081772  0.80632061 0.80693678\n",
      " 0.80754268 0.8075094  0.80767714 0.80803447 0.80826335 0.8076534\n",
      " 0.80813786 0.80719961 0.80802339 0.80639986 0.8069752  0.80782744\n",
      " 0.80827838 0.80639783        nan 0.80756241 0.80779559 0.80760689\n",
      " 0.80794207 0.80827846        nan 0.80810297 0.80677626 0.80660176\n",
      " 0.80794803 0.80823074 0.80641449 0.8077884  0.80753713 0.80664038\n",
      " 0.80671811 0.80745888 0.80750601 0.80782214 0.80631021 0.80777908\n",
      " 0.80713494 0.80801019 0.80668753 0.80747828 0.80740978 0.80784984\n",
      " 0.80639139 0.80713771        nan 0.80643629 0.80726358 0.80655407\n",
      " 0.80641158 0.80806513 0.80792394        nan 0.80828493 0.80797123\n",
      " 0.80665781 0.80826971 0.80699611 0.80697235 0.80795414 0.8075048\n",
      " 0.80798637        nan 0.80773044 0.8076752  0.80819205 0.80764243\n",
      " 0.80713855 0.80813253 0.80647361 0.8077843  0.80635275 0.80690522\n",
      " 0.80655127 0.80770401 0.80743099 0.80738109 0.80730637 0.80772656\n",
      " 0.80749027 0.8071774  0.80734436 0.80734366 0.80737785 0.80730475\n",
      " 0.80810717        nan        nan 0.80777563 0.80699608 0.80692446\n",
      " 0.80826119 0.80707428 0.80812293 0.80825014 0.80673727        nan\n",
      " 0.80638714 0.8072269  0.80761287 0.80665188 0.80829822 0.80745651\n",
      " 0.80692139 0.80766545 0.80815659 0.80757494 0.80712841 0.80757308\n",
      " 0.8082339  0.80785714 0.80773173 0.80740849 0.80702489 0.80704052\n",
      " 0.80737664 0.80700115 0.8082946  0.80670941 0.80642761 0.80743223\n",
      " 0.8074744  0.80648988 0.80795105 0.80709239 0.80829528 0.80699369\n",
      " 0.80651394 0.80715555 0.80757286 0.80762093        nan 0.80679445\n",
      " 0.80760299 0.80786832 0.80668106 0.80765504 0.80703742 0.80639754\n",
      " 0.80799737 0.80730014 0.80735366 0.80758189 0.8072604  0.80825138\n",
      " 0.80699366 0.80681428 0.80760401 0.80825591 0.80748464 0.80796659\n",
      " 0.80819816 0.80708325 0.80809421 0.80725231 0.80739348 0.80655302\n",
      "        nan 0.80772128 0.8067685  0.80744018 0.80740545 0.80667233\n",
      " 0.80748717 0.8073065  0.80801262 0.80773338 0.80834955 0.80760102\n",
      " 0.80638312 0.80812115 0.80827345 0.80758582 0.80766078 0.80785792\n",
      " 0.80730464 0.8080553  0.80753921 0.80783259 0.8082897  0.80709188\n",
      " 0.80651826 0.80718349 0.80669812 0.8073224  0.80658263 0.80793808\n",
      " 0.80630743 0.80773165 0.80732666 0.8077625  0.80729971 0.80674694\n",
      " 0.80667683 0.80787813 0.80677874 0.80830862 0.80823247 0.80697639\n",
      " 0.80763893 0.80754362 0.80812468 0.8080608  0.8066937  0.80812296\n",
      "        nan 0.80818348 0.80719826 0.80733355 0.80686728 0.80757566\n",
      " 0.80819984 0.8071798  0.80676893 0.80804377 0.80788015 0.80795053\n",
      " 0.80746155 0.80751738 0.80834844 0.80652386 0.80811264 0.80782955\n",
      " 0.80750267 0.8067862  0.80778444 0.80746648 0.80720804 0.80816198\n",
      " 0.80822177 0.80712373 0.80792316 0.80829741 0.80706501 0.80708625\n",
      " 0.80767975 0.80772947 0.80741474 0.80751371 0.80796196        nan\n",
      " 0.80719249 0.80823928 0.80654062 0.8077458  0.80808804 0.80804506\n",
      " 0.80636722 0.80825394 0.8078642  0.80792483 0.8072338  0.80771988\n",
      " 0.80832225 0.80753171 0.80803312 0.80772244 0.80638956 0.80817028\n",
      " 0.80631649 0.80701241 0.80716638 0.8074567  0.80786563 0.8079127\n",
      " 0.80831185 0.80795999 0.80727247 0.80631573 0.80643995 0.8066307\n",
      " 0.80747747 0.80792035 0.8065556  0.80633457 0.80784342 0.80716102\n",
      "        nan 0.80784361        nan 0.80771945 0.80810844 0.80753958\n",
      " 0.80646337 0.8072809  0.80804899 0.80675007 0.80762451 0.80813476\n",
      " 0.80780616 0.80788053 0.80714798 0.80814285        nan 0.80794819\n",
      " 0.80790036 0.80718737 0.80722803 0.8083253  0.80725635 0.80809348\n",
      " 0.80824009 0.8077932  0.80807386 0.80723504 0.80780791 0.80651861\n",
      " 0.80769905 0.8069966  0.80687011 0.80833777 0.80694818        nan\n",
      " 0.80760519 0.8074903  0.80821299 0.80648888 0.80655498 0.80635367\n",
      " 0.80634896 0.80834723 0.80755071 0.80673762 0.80782995 0.80738208\n",
      " 0.80741008 0.80694836 0.80640789 0.80724019 0.80814735 0.80808459\n",
      " 0.80783529 0.80651944 0.80833036 0.80711249 0.80743783 0.80815392\n",
      " 0.8083253  0.80732843 0.80762408 0.8065725  0.80690921 0.80829048\n",
      " 0.80635558 0.80729521 0.80696378 0.80708837 0.8065067  0.80636518\n",
      " 0.80756373 0.80695052 0.80765901 0.80781017 0.80784652 0.80798861\n",
      " 0.80834012 0.80732431 0.80782478 0.8081882  0.80829533 0.80786662\n",
      " 0.807725   0.80813465 0.80724226 0.8064974  0.80832222        nan\n",
      " 0.80737589 0.80832961 0.80770926 0.80697986 0.80803703 0.80713205\n",
      " 0.80724299 0.80752835 0.80768318 0.80810132 0.80821969 0.80762972\n",
      " 0.80753522 0.80831177 0.80822153 0.8075454  0.80777363 0.80802989\n",
      " 0.80829803 0.80756224 0.80724296 0.80705846 0.806575   0.80705302\n",
      " 0.80767143 0.80737729 0.80832185 0.80780815 0.80748095 0.80793733\n",
      " 0.80707353 0.80832201 0.80792655 0.8080387  0.80807152 0.80807613\n",
      " 0.80795226 0.80696653 0.80758202 0.80833853 0.80811259 0.80714946\n",
      " 0.8083396  0.80828534 0.80801173 0.80809892 0.80690657 0.80726816\n",
      " 0.80797751 0.80685408        nan 0.80827709 0.80710602 0.8074353\n",
      " 0.80663531 0.80796632 0.80792216 0.80713669 0.80778789 0.80765176\n",
      "        nan 0.80706065 0.8074097  0.80715773 0.80677809 0.80807764\n",
      " 0.80698401 0.80697914 0.80798858 0.80671305        nan 0.80776067\n",
      " 0.80680385 0.80767234 0.8070579  0.80826154 0.80774154 0.80699908\n",
      " 0.80678138 0.80649045 0.80655563 0.80676074        nan 0.80710066\n",
      " 0.80734579 0.80688324 0.80717121 0.80739195 0.80774076 0.80673361\n",
      " 0.80806279 0.80699762        nan 0.80644469 0.80787468 0.80832309\n",
      " 0.80702869 0.80756071 0.80655369 0.80811862 0.80712327 0.80645752\n",
      " 0.80664215 0.80790591 0.80803142        nan 0.80677642 0.80794655\n",
      " 0.80750442 0.80679941 0.8066795  0.80736702 0.80659255 0.80833659\n",
      " 0.807178   0.80741073 0.80806705 0.80730655 0.80811574 0.80688181\n",
      " 0.80771354 0.80737581 0.8079594  0.8082499  0.80671825 0.80646094\n",
      " 0.80765036 0.80683764 0.80761891 0.80678195 0.80833734 0.80821414\n",
      " 0.8081156  0.80798198 0.80648754 0.80779635 0.80715851 0.80744578\n",
      " 0.80676966 0.80638342 0.80815333 0.80721308 0.80802205 0.80736796\n",
      " 0.80669472        nan 0.80665447 0.8069277  0.80657754 0.80644383\n",
      " 0.80710241 0.80786153 0.80739844 0.80788144 0.80753993 0.8073567\n",
      " 0.80812048 0.80833667 0.80665827 0.80798675 0.80662811 0.808135\n",
      " 0.80640422 0.80799763 0.80729039 0.80691185 0.80714652 0.80689851\n",
      " 0.80815387 0.80745982 0.80835157 0.80771279 0.80811412 0.80749755\n",
      " 0.80724267 0.80751091 0.80797955 0.80822754 0.8077046  0.80791955\n",
      "        nan 0.80803062 0.80821509 0.80834914 0.80820415 0.80666395\n",
      " 0.8078839  0.80754174 0.80786245 0.80641993 0.8075696  0.80753112\n",
      " 0.80821196 0.80819868 0.80823088 0.80822188 0.80706329 0.80783335\n",
      " 0.80697604 0.80718732 0.80663817 0.80667664 0.80743926 0.80677901\n",
      " 0.80719931 0.80792976 0.80834065 0.8078611  0.80712009 0.80780511\n",
      "        nan 0.80808308 0.80742328 0.80813646 0.80700177 0.80764386\n",
      " 0.80765981 0.8064908  0.80700312 0.80642877 0.80780551 0.80723076\n",
      " 0.80697585 0.80825825 0.8066899  0.80834033 0.80789087 0.8075405\n",
      " 0.80762107 0.80670114 0.80789214 0.80819873 0.80834001 0.80756284\n",
      " 0.80644084 0.80804487 0.80776808 0.80779263 0.80705229 0.80785776\n",
      " 0.80740116 0.80787099 0.80829301 0.80818661 0.80821296 0.80646218\n",
      "        nan 0.80708883 0.80735794 0.80683791 0.80762457 0.80747017\n",
      " 0.80788255 0.80784849 0.80738373 0.80829169 0.80652629 0.80782672\n",
      " 0.80832195 0.80801787 0.8079618  0.80796376 0.80637186 0.80724345\n",
      " 0.80757906 0.80811886 0.80815899 0.80787215 0.80832228 0.80792593\n",
      " 0.8072193  0.80792337 0.80772801 0.80823934 0.80636957        nan\n",
      " 0.807356          nan 0.80668597 0.8078054  0.80763664 0.80780624\n",
      " 0.80763653 0.80746696 0.8064798  0.80777264 0.80655245 0.80821263\n",
      " 0.80802649 0.80756647 0.80721028 0.80756119 0.80823462 0.80738084\n",
      " 0.80782941 0.80821603 0.80698568 0.80788244 0.80751776 0.80664803\n",
      " 0.8073861  0.80762918 0.80656865 0.80822503 0.80832788 0.8081619\n",
      " 0.80818173 0.8082101  0.80770438        nan 0.8078756  0.80727834\n",
      " 0.80675732 0.80824748 0.80789502 0.806828   0.80673673 0.80809712\n",
      " 0.80827523 0.80804986 0.80823864 0.80792402 0.80639856 0.8081453\n",
      " 0.80833112 0.80749569 0.80760916 0.80802862 0.80792601 0.80756674\n",
      " 0.80746392 0.80699272 0.80738454 0.80812382 0.80741851 0.8081273\n",
      "        nan 0.80824333 0.80668395 0.80692339 0.8066106  0.80696052\n",
      " 0.80816914 0.806539   0.8065756  0.80773402 0.80770301 0.80679865\n",
      " 0.80797815 0.8069004  0.80757914 0.8078182  0.80783725 0.80632875\n",
      " 0.8076472  0.80805638 0.80831379 0.80655789 0.80720109 0.80742048\n",
      " 0.80774995 0.80783812 0.80799192 0.80832934 0.80684759 0.8074013\n",
      " 0.8082616  0.80822164 0.80668788 0.8069278  0.80665479 0.80635634\n",
      " 0.80760555 0.8083506  0.8073176  0.80680202 0.8078618  0.80790475\n",
      " 0.8074287  0.80667131 0.80760864 0.8067854  0.80809469 0.80829258\n",
      "        nan 0.80770735 0.80727522 0.80820027 0.80649312 0.80825322\n",
      " 0.80775041 0.80783962 0.8074072  0.80727551 0.80738165 0.80835771\n",
      " 0.80700692 0.80712346 0.80766351 0.80670469 0.8075668  0.80643704\n",
      " 0.8063344  0.80819593 0.80663049 0.80694362 0.80773346 0.80807157\n",
      " 0.80758962 0.8065241  0.80826418 0.80732994 0.80768148 0.80794843\n",
      " 0.80639765 0.80770392 0.80786854 0.80769266 0.8064108  0.80666061\n",
      " 0.80762807 0.80668147 0.80666859 0.80804724]\n",
      "  warnings.warn(\n",
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'list' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18812/3028997092.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m } \n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mfeatures_heat_scatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18812/551349522.py\u001b[0m in \u001b[0;36mfeatures_heat_scatter\u001b[1;34m(clf, param_grid, n_iter, X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mvariables\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mdependent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mindependent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '>' not supported between instances of 'list' and 'int'"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "clf = LogisticRegression(solver = 'saga', penalty='elasticnet')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'C': np.unique(np.linspace(0, 2, 21).round(1)), \n",
    "    'max_iter': list(range(500,1500)),\n",
    "    'l1_ratio': np.unique(np.linspace(0, 1, 11).round(1))\n",
    "} \n",
    "\n",
    "features_heat_scatter(clf, param_grid, 1000, X_train, y_train, X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18812/1664341010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures_heat_scatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18812/551349522.py\u001b[0m in \u001b[0;36mfeatures_heat_scatter\u001b[1;34m(clf, param_grid, n_iter, X_train, y_train, X_test)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Fit to the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mrand_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# Create dataframe of random grid search results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1765\u001b[0m         \u001b[1;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1766\u001b[1;33m         evaluate_candidates(\n\u001b[0m\u001b[0;32m   1767\u001b[0m             ParameterSampler(\n\u001b[0;32m   1768\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\andre\\AppData\\Local\\Programs\\Python\\Python39\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "features_heat_scatter(clf, param_grid, 1000, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest Classifier with specified criterion\n",
    "clf = RandomForestClassifier(criterion='entropy')\n",
    "\n",
    "# Create the parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': list(range(20,100)), \n",
    "    'min_samples_leaf': list(range(1,20)),\n",
    "    'n_estimators':list(range(100, 500))\n",
    "} \n",
    "\n",
    "features_heat_scatter(clf, param_grid, 1000, X_train, y_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('data/test.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "08e595c52ca3b9470036b1110e67b559e55f367cabc363f2e28d35631ed95060"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
